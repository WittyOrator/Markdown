# ç¬¬10è¯¾: å˜åˆ†è‡ªç¼–ç å™¨(VAE)

> [CS20siè¯¾ç¨‹èµ„æ–™å’Œä»£ç Githubåœ°å€](https://github.com/cnscott/Stanford-CS20si)

<!-- TOC -->

- [ç¬¬10è¯¾: å˜åˆ†è‡ªç¼–ç å™¨(VAE)](#ç¬¬10è¯¾-å˜åˆ†è‡ªç¼–ç å™¨vae)
    - [å˜åˆ†æ¨æ–­(Variational Inference)](#å˜åˆ†æ¨æ–­variational-inference)
        - [å­¦ä¹ æœªçŸ¥çš„å˜é‡](#å­¦ä¹ æœªçŸ¥çš„å˜é‡)
        - [å˜åˆ†ä¸‹ç•Œ: æ¦‚è§ˆ](#å˜åˆ†ä¸‹ç•Œ-æ¦‚è§ˆ)
        - [å˜åˆ†ä¸‹ç•Œ: ç®—æ³•](#å˜åˆ†ä¸‹ç•Œ-ç®—æ³•)
        - [åˆ†æ®µæ¨æ–­: æ¦‚è§ˆ](#åˆ†æ®µæ¨æ–­-æ¦‚è§ˆ)
        - [åˆ†æ®µæ¨æ–­: ç®—æ³•](#åˆ†æ®µæ¨æ–­-ç®—æ³•)
        - [å˜åˆ†è‡ªç¼–ç å™¨(VAE, Variational Auto-Encoder)](#å˜åˆ†è‡ªç¼–ç å™¨vae-variational-auto-encoder)
        - [è´å¶æ–¯ç¥ç»ç½‘ç»œ](#è´å¶æ–¯ç¥ç»ç½‘ç»œ)
    - [TensorFlowå®ç°](#tensorflowå®ç°)
        - [TensorFlowä¸­çš„æ¦‚ç‡åˆ†å¸ƒ: æ¦‚è§ˆ](#tensorflowä¸­çš„æ¦‚ç‡åˆ†å¸ƒ-æ¦‚è§ˆ)
        - [TensorFlowä¸­çš„æ¦‚ç‡åˆ†å¸ƒ: å›å½’çš„ä¾‹å­](#tensorflowä¸­çš„æ¦‚ç‡åˆ†å¸ƒ-å›å½’çš„ä¾‹å­)
        - [TensorFlowä¸­çš„æ¦‚ç‡åˆ†å¸ƒ: åˆ†ç±»çš„ä¾‹å­](#tensorflowä¸­çš„æ¦‚ç‡åˆ†å¸ƒ-åˆ†ç±»çš„ä¾‹å­)
        - [VAEçš„TensorFlowå®ç°: æ¦‚è§ˆ](#vaeçš„tensorflowå®ç°-æ¦‚è§ˆ)
        - [VAEçš„TensorFlowå®ç°: Prior & encoder](#vaeçš„tensorflowå®ç°-prior--encoder)
        - [VAEçš„TensorFlowå®ç°: ç½‘ç»œ](#vaeçš„tensorflowå®ç°-ç½‘ç»œ)
        - [VAEçš„TensorFlowå®ç°: ç»“æœ](#vaeçš„tensorflowå®ç°-ç»“æœ)
        - [è´å¶æ–¯ç½‘ç»œçš„TensorFlowå®ç°](#è´å¶æ–¯ç½‘ç»œçš„tensorflowå®ç°)

<!-- /TOC -->

## å˜åˆ†æ¨æ–­(Variational Inference)
å…³äºå˜åˆ†æ¨æ–­çš„åŸºç¡€æ¦‚å¿µå¯ä»¥å…ˆå‚ç…§[è¿™ç¯‡æ–‡ç« ](https://www.cnblogs.com/yifdu25/p/8181185.html)ã€‚

### å­¦ä¹ æœªçŸ¥çš„å˜é‡

å›¾åƒç”±æ•°ç™¾ä¸‡åƒç´ ç»„æˆï¼Œä½†å¯èƒ½æœ‰æ›´ç´§å‡‘çš„å†…å®¹è¡¨ç¤ºæ–¹å¼(å¯¹è±¡ã€ä½ç½®ç­‰)ã€‚

æ‰¾åˆ°è¿™ä¸ªè¡¨ç¤ºçš„æ˜ å°„å¯ä»¥ä½¿æˆ‘ä»¬æ‰¾å‡ºè¯­ä¹‰ç›¸ä¼¼çš„å›¾åƒï¼Œç”šè‡³ç”Ÿæˆæ–°çš„å›¾åƒã€‚

æˆ‘ä»¬å«è¿™ç§ç´§å‡‘è¡¨ç¤ºä¸ºzï¼Œå®ƒå¯¹åº”çš„åƒç´ ç‚¹é›†åˆä¸ºxã€‚æˆ‘ä»¬çš„Nå¼ å›¾ç‰‡éƒ½æ˜¯ä¸€æ ·çš„ç»“æ„ã€‚

![](http://images.cnblogs.com/cnblogs_com/tech0ne/1247403/o_Dist-z-x.png)


å‡è®¾æ¨¡å‹ä¸­çš„xç”±éšå«å˜é‡zç”Ÿæˆï¼Œæˆ‘ä»¬æƒ³è¦æ‰¾åˆ°è§£é‡Šæ•°æ®çš„éšå«å˜é‡ã€‚

![](http://images.cnblogs.com/cnblogs_com/tech0ne/1247403/o_Map-z-x.png)

ä½†æ˜¯æˆ‘ä»¬ä¸èƒ½ç›´æ¥è®¡ç®—æ•°æ®çš„æœ€å¤§ä¼¼ç„¶ï¼Œå› ä¸ºå®ƒå–å†³äºéšå«å˜é‡zè€Œæˆ‘ä»¬ä¸çŸ¥é“å®ƒä»¬çš„å€¼ã€‚

$$Î¸* = argmax P_Î¸(x) = argmax âˆ« P_Î¸(x|z)P(z) dz$$

æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå…ˆéªŒå‡è®¾P(z)ï¼Œå®ƒæ˜¯zçš„åˆ†å¸ƒã€‚

æˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯åéªŒæ¦‚ç‡P(z|x)ï¼Œå®ƒä¾èµ–äºç›¸åº”çš„æ•°æ®ç‚¹xã€‚

### å˜åˆ†ä¸‹ç•Œ: æ¦‚è§ˆ

è¿­ä»£ä¼˜åŒ–è¿‘ä¼¼çš„åéªŒæ¦‚ç‡Q(z)ç›´åˆ°Q(z) â‰ˆ P(z|x)

![](http://images.cnblogs.com/cnblogs_com/tech0ne/1247403/o_Q-z.png)

Q(z)çš„ç›®æ ‡æ˜¯å˜åˆ†ä¸‹ç•Œ

$$\begin{aligned}
lnP(x) &â‰¥ E_{Q(z)}[lnP(x,z) âˆ’ lnQ(z)] \\
&= E_{Q(z)}[lnP(x|z) + lnP(z) âˆ’ lnQ(z)] \\
&= E_{Q(z)}[lnP(x|z)] âˆ’ D_{KL}[Q(z)||P(z)]
\end{aligned}$$

è¿™æ˜¯ä¸€ä¸ªä¸‹ç•Œå› ä¸ºKLæ•£åº¦æ˜¯éè´Ÿçš„ã€‚

å– Q(z) Ïµ ğ’¬ ä¸ºå¯å¾®çš„é‡‡æ ·ï¼Œå¸¸å¸¸å–é«˜æ–¯åˆ†å¸ƒã€‚KLæ•£åº¦æ˜¯ä¸€ä¸ªæ­£åˆ™é¡¹ã€‚

### å˜åˆ†ä¸‹ç•Œ: ç®—æ³•

å°†éšå«çš„P(z)åˆå§‹åŒ–ä¸ºå›ºå®šçš„å…ˆéªŒæ¦‚ç‡ï¼Œæ¯”å¦‚è¯´0å‡å€¼å•ä½æ–¹å·®çš„é«˜æ–¯åˆ†å¸ƒã€‚

åˆå§‹åŒ–ç½‘ç»œæƒå€¼ Î¸ å’Œ Q(z) çš„ Î¼ å’Œ Ïƒ ã€‚

è®°ä½ç›®æ ‡å‡½æ•° $lnP(x) â‰¥ E_{Q(z)}[lnP_Î¸(x|z)] âˆ’ D_{KL}[Q(z)||P(z)]$

è¿­ä»£ç›´åˆ°æ”¶æ•›ï¼š

1. ä»Q(z)ä¸­å–æ ·ä¸€ä¸ª$z$ï¼Œ$z = ÏƒÎµ + Î¼$ &nbsp;&nbsp;(Îµ ~ N(0, 1))
2. ç”¨ç¥ç»ç½‘ç»œè®¡ç®— $P_Î¸(x|z)$
3. è®¡ç®—$Q(z)$å’Œ$P(z)$çš„KLæ•£åº¦
4. è®¡ç®—ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦æ¥ä¼˜åŒ– $Î¸, Î¼, Ïƒ$

### åˆ†æ®µæ¨æ–­: æ¦‚è§ˆ

æˆ‘ä»¬å¯ä»¥é€šè¿‡æ¢¯åº¦ä¸‹é™çš„æ–¹æ³•ï¼Œä¸ºæ¯ä¸ªæ•°æ®ç‚¹å­¦ä¹ è¶³å¤Ÿçš„Q(z)ç»Ÿè®¡é‡ã€‚ä½†æ˜¯æ¯ä¸ªæ•°æ®ç‚¹éƒ½éœ€è¦å¤šä¸ªæ±‚æ¢¯åº¦çš„æ­¥éª¤ï¼Œå³ä½¿æ˜¯åœ¨è¯„ä»·æ—¶ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªç¼–ç å™¨ç½‘ç»œQ(z|x)å­¦ä¹ è¿™ä¸ªè¿‡ç¨‹çš„ç»“æœã€‚æƒ³è±¡ä¸€ä¸‹æ€æ ·ä¸ºæ‰€æœ‰çš„æ•°æ®ç‚¹æ¨æ–­éšå«å˜é‡ï¼Œåå‘ä¼ æ’­ä¼˜åŒ–ç¼–ç å™¨çš„æƒé‡ï¼Œè€Œä¸æ˜¯åéªŒçš„ç»Ÿè®¡é‡$Î¼,Ïƒ$ã€‚

### åˆ†æ®µæ¨æ–­: ç®—æ³•

å°†éšå«çš„P(z)åˆå§‹åŒ–ä¸ºå›ºå®šçš„å…ˆéªŒæ¦‚ç‡ï¼Œæ¯”å¦‚è¯´0å‡å€¼å•ä½æ–¹å·®çš„é«˜æ–¯åˆ†å¸ƒã€‚

åˆå§‹åŒ–ç¼–ç å™¨çš„æƒå€¼ Ï• å’Œè§£ç å™¨çš„æƒå€¼ Î¸ ã€‚

è®°ä½ç›®æ ‡å‡½æ•° $lnP(x) â‰¥ E_{Q(z)}[lnP_Î¸(x|z)] âˆ’ D_{KL}[Q_Ï•(z|x)||P(z)]$

è¿­ä»£ç›´åˆ°æ”¶æ•›ï¼š

1. é€‰æ‹©æ•°æ®ç‚¹xå¹¶ç”¨ç¼–ç å™¨è®¡ç®— $QÏ•(z|x)$ ã€‚
2. ä»Q(z|x)ä¸­å–æ ·ä¸€ä¸ª$z$ï¼Œ$z = ÏƒÎµ + Î¼$ &nbsp;&nbsp;(Îµ ~ N(0, 1))
3. ç”¨è§£ç å™¨è®¡ç®— $P_Î¸(x|z)$
4. è®¡ç®—$Q(z)$å’Œ$P(z)$çš„KLæ•£åº¦
5. è®¡ç®—ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦æ¥ä¼˜åŒ– $Î¸, Ï•$

### å˜åˆ†è‡ªç¼–ç å™¨(VAE, Variational Auto-Encoder)
ç¼–ç å™¨ç”¨æ¥åˆ†æ®µæ¨æ–­å‡ºQ(z|x), è§£ç å™¨ç”¨æ¥ç”Ÿæˆæ¨¡å‹P(x|z)ã€‚
å˜åˆ†ä¸‹ç•Œç›®æ ‡å‡½æ•° $E_{Q(z|x)}[lnP(x|z)] âˆ’ D_{KL}[Q(z|x)||P(z)]$ã€‚

é€šè¿‡æ¢¯åº¦ä¸‹é™è®­ç»ƒç«¯åˆ°ç«¯æ¨¡å‹ã€‚

![](http://images.cnblogs.com/cnblogs_com/tech0ne/1247403/o_VAE-Model.png)

### è´å¶æ–¯ç¥ç»ç½‘ç»œ
ç‹¬ç«‹éšå«çš„Q(Î¸)æ˜¯å¯¹è§’é«˜æ–¯åˆ†å¸ƒã€‚

æ¡ä»¶ç”Ÿæˆæ¨¡å‹$P_Î¸(y|x)$ã€‚

![](http://images.cnblogs.com/cnblogs_com/tech0ne/1247403/o_Bayes-x-y.png)

å˜åˆ†ä¸‹ç•Œç›®æ ‡å‡½æ•°ï¼š$E_{Q(Î¸)}[lnP_Î¸(y|x)] âˆ’ D_{KL}[Q(Î¸)||P(Î¸)]$

å°†KLé¡¹é™¤ä»¥æ•°æ®é›†å¤§å°ï¼Œå› ä¸ºæ•´ä¸ªæ•°æ®é›†çš„å‚æ•°æ˜¯å…±äº«çš„ã€‚

é€šè¿‡æ¢¯åº¦ä¸‹é™è®­ç»ƒä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¨¡å‹ã€‚

![](http://images.cnblogs.com/cnblogs_com/tech0ne/1247403/o_Bayes-Model.png)

## TensorFlowå®ç°

### TensorFlowä¸­çš„æ¦‚ç‡åˆ†å¸ƒ: æ¦‚è§ˆ

åœ¨TensorFlowä¸­æ¦‚ç‡ç¼–ç¨‹å¾ˆå®¹æ˜“ï¼

Probabilistic programming made easy!

    tfd = tf.contrib.distributions

    mean = tf.layers.dense(hidden, 10, None)
    stddev = tf.layers.dense(hidden, 10, tf.nn.softplus)
    dist = tfd.MultivariateNormalDiag(mean, stddev)

    samples = dist.sample()
    dist.log_prob(samples)

    other = tfd.MultivariateNormalDiag(
        tf.zeros_like(mean), tf.ones_like(stddev))
    tfd.kl_divergence(dist, other)

### TensorFlowä¸­çš„æ¦‚ç‡åˆ†å¸ƒ: å›å½’çš„ä¾‹å­

    tfd = tf.contrib.distributions
    hidden = tf.layers.dense(inputs, 100, tf.nn.relu)
    mean = tf.layers.dense(hidden, 10, None)
    dist = tfd.MultivariateNormalDiag(mean, tf.ones_like(mean))
    loss = -dist.log_prob(label)  # Squared error
    optimize = tf.train.AdamOptimizer().minimize(loss)

### TensorFlowä¸­çš„æ¦‚ç‡åˆ†å¸ƒ: åˆ†ç±»çš„ä¾‹å­

    tfd = tf.contrib.distributions
    hidden = tf.layers.dense(inputs, 100, tf.nn.relu)
    logit = tf.layers.dense(hidden, 10, None)
    dist = tfd.Categorical(logit)
    loss = -dist.log_prob(label)  # Cross entropy
    optimize = tf.train.AdamOptimizer().minimize(loss)

### VAEçš„TensorFlowå®ç°: æ¦‚è§ˆ

    tfd = tf.contrib.distributions
    images = tf.placeholder(tf.float32, [None, 28, 28])
    prior = make_prior()
    posterior = make_encoder(images)
    dist = make_decoder(posterior.sample())
    elbo = dist.log_prob(images) - tfd.kl_divergence(posterior, prior)optimizer = tf.train.AdamOptimizer().minimize(-elbo)
    samples = make_decoder(prior.sample(10)).mean()  # For visualization

### VAEçš„TensorFlowå®ç°: Prior & encoder

    def make_prior(code_size=2):
        mean, stddev = tf.zeros([code_size]), tf.ones([code_size])
        return tfd.MultivariateNormalDiag(mean, stddev)

    def make_encoder(images, code_size=2):
        images = tf.layers.flatten(images)
        hidden = tf.layers.dense(images, 100, tf.nn.relu)
        mean = tf.layers.dense(hidden, code_size)
        stddev = tf.layers.dense(hidden, code_size, tf.nn.softplus)
        return tfd.MultivariateNormalDiag(mean, stddev)

### VAEçš„TensorFlowå®ç°: ç½‘ç»œ

    def make_decoder(code, data_shape=[28, 28]):
        hidden = tf.layers.dense(code, 100, tf.nn.relu)
        logit = tf.layers.dense(hidden, np.prod(data_shape))
        logit = tf.reshape(logit, [-1] + data_shape)
        return tfd.Independent(tfd.Bernoulli(logit), len(data_shape))

`tfd.Independent(dist, 2)`å‘Šè¯‰TensorFlowå°†æœ€åä¸¤ç»´è§†ä¸ºæ•°æ®ç»´åº¦ï¼Œè€Œä¸æ˜¯æ‰¹å¤„ç†ç»´åº¦ã€‚

è¿™è¯´æ˜`dist.log_prob(images)`å¯¹æ¯å¼ å›¾ç‰‡è¿”å›ä¸€ä¸ªæ•°å­—è€Œä¸æ˜¯æ¯ä¸ªç‚¹ã€‚ 

æ­£å¦‚åç§°`tfd.independent()`æ‰€è¡¨ç¤ºï¼Œå®ƒåªæ˜¯å°†åƒç´ å¯¹æ•°æ¦‚ç‡ç›¸åŠ ã€‚

### VAEçš„TensorFlowå®ç°: ç»“æœ

![](http://images.cnblogs.com/cnblogs_com/tech0ne/1247403/o_VAE-Result.png)

### è´å¶æ–¯ç½‘ç»œçš„TensorFlowå®ç°

    def define_network(images, num_classes=10):
        mean = tf.get_variable('mean', [28 * 28, num_classes])
        stddev = tf.get_variable('stddev', [28 * 28, num_classes])
        prior = tfd.MultivariateNormalDiag(
            tf.zeros_like(mean), tf.ones_like(stddev))
        posterior = tfd.MultivariateNormalDiag(mean, tf.nn.softplus(stddev))
        bias = tf.get_variable('bias', [num_classes])  # Or Bayesian, too
        logit = tf.nn.relu(tf.matmul(posterior.sample(), images) + bias)
        return tfd.Categorical(logit), posterior, prior
    dist, posterior, prior = define_network(images)
    elbo = (tf.reduce_mean(dist.log_prob(label)) -
            tf.reduce_mean(tfd.kl_divergence(posterior, prior))