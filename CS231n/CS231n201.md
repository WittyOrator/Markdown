#CS231n 2018 课程笔记 模块二 01#
[卷积神经网络：架构，卷积/池化层](http://cs231n.github.io/convolutional-networks/)
----------
这一部分主要讨论建立网络之后如何进行学习，内容包括：

1. **CNNs架构概览**
2. **CNNs各层介绍**
3. **CNNs框架介绍**
4. **附加参考**

#卷积神经网络（CNNs/ConvNets)#
卷积神经网络与普通的神经网络非常类似：它们都是由神经元组成并拥有可以训练的权值和偏差。整个网络仍然表示一个可导的评分函数：从原始的图片像素开始到各类型的得分结束。在最后的输出层它们也都有损失函数（比如SVM或Softmax），并且普通神经网络上的技巧同样能在CNNs中应用。

那么什么改变了呢？CNNs假设输入都是图片，这样允许我们在架构上能对特定的属性编码，这些变化能够实现和大量减少网络中的参数使前向函数更有效率。

##1.架构概览##
回忆之前课程所讲的常规神经网络，网络接收一个向量输入，然后通过多个隐层对其进行变换。每个隐层由一组神经元组成，其中的每个神经元都与前一层的所有神经元进行全连接，它们的功能都是独立的且不共享任何连接。最后一个被全连接的层叫做输出层，在分类应用中它表现为各类型得分。

常规的神经网络不能适应完全的图片数据。在CIFAR-10中，图片的大小只有32x32x3(32像素宽，32像素高，3个颜色通道)，所以常规神经网络中在首个隐层中的一个全连接神经元应该有32*32*3=3072个权值。这个数量看上去可以管理，但是很明显全连接结构不能适应更大的图片。例如一个更大分辨率的图片（200x200x3）会使神经元拥有200*200*3=120000个权值。进一步说，我们显然需要一些这样的神经元，所以参数数量会快速的上升。很明显这种情况下全连接是没用的，而且巨量的参数会快速导致过拟合。

三维神经元排列。CNNs改变了输入是由图片组成的事实并使用了一种更明智的架构。CNNs层中的神经元按3个维度排列而成：**宽度，高度，深度**。（注意这里的深度不是网络层数）。我们很快会看到在一个CNNs层中的神经元只连接上一层的一小块区域而不是全连接。进一步说，分类CIFAR-10的输出层的纬度为1x1x10，因为在CNNs架构的最后我们将完全的图片缩减成了一个各类型得分的向量。

![](http://cs231n.github.io/assets/nn1/neural_net2.jpeg)

普通的三层神经网络。

![](http://cs231n.github.io/assets/cnn/cnn.jpeg)

一个CNNs将它的神经元排列成三维结构，每一层将3D数据块转换为3D输出块。在这幅图中，宽和高是图片的分辨率而深度是图片的颜色通道数（红绿蓝）

**一个CNNs由多个层组成，每个层拥有一个简单的API：将输入3D块用可导的函数转换为输出3D块，函数中可能有参数也可能没有。**

##2.CNNs中的各种层##
如上锁描述，一个简单的CNNs是由一连串层组成，每个层通过一个可导函数将一个块转换为另一个块。我们用三个主要类型的层来构建CNNs：**卷积层（Convolutional Layer），池化层（Pooling Layer）和 全连接层（Fully-Connected Layer）**。我们将会堆叠这些层来形成一个完整的CNNs架构。

样例架构概览。我们将会在下面涉及更多细节，但是一个简单的分类CIFAR-10的CNNs应该有这种架构[输入层-卷积层-RELU层-池化层-全连接层]，具体如下：

- 输入层（**INPUT**）[32x32x3]将会持有原始的图片像素（32像素宽，32像素高，3个颜色通道RGB）
- 卷积层（**CONV**）将会计算输入中的局部区域的输出，每一个层计算它的权值和它连接的局部区域的点积。结果可能是一个大小为[32x32x12]的块，如果我们选择用12个滤波器。
- ReLU层（**RELU**）将应用激活函数处理输入，比如\\(max(0,x)\\),输出大小不变为[32x32x12]。
- 池化层（**POOL**）将对空间维度（宽和高）做下采样操作，结果大小可能是[16x16x12]。
- 全连接层（**FC**）将会计算出各类型得分，结果的块大小为[1x1x10]。像它的名字一样，这一层的所有神经元都会和上一层的所有神经元相连。

在这种方式下，CNNs通过一层又一层操作将原始图片像素值转换为最终的类型得分。注意一些层是有参数的而另一些是没有参数的。具体来说，卷积层和全连接层的变换操作不仅是激活输入块，还包括其它参数（神经元的权重和偏差），而ReLU层和池化层只进行固定的操作。卷积层和全连接层中的参数通过梯度下降进行训练使得CNNs算出的类型得分和训练集中图片的真实类型标签一致。

概括：

- 一个CNNs架构的最简单的例子是将图像块通过一连串层转换为输出块（比如类型得分向量）
- 其中有一些不同类型的层（例如CONV/FC/RELU/POOL是最常见的，大写表示层）
- 每一层接受一个3D块输入然后通过一个可导函数转换为一个3D块输出
- 每一层有或没有参数（例如CONV/FC有，RELU/POOL没有）
- 每一层有或没有附加的超参数（例如CONV/FC/POOL有，RELU没有）

![](http://cs231n.github.io/assets/cnn/convnet.jpeg)

这是一个CNNs架构的例子，最左边的初始块存储原始图片像素，最右边的输出块存储类型得分，它只显示了得分最高的5种类型。接下来我们将描述各个独立的层和各层中的细节。

###2.1 卷积层（CONV）###
卷积层是CNNs的核心构建模块，它承担了绝大多数计算任务。

**不涉及大脑分析的简介** 让我们首先不从分析大脑神经元的角度来讨论CONV，它的参数由一组可学习的滤波器组成。每一个滤波器在空间维度上很小（比输入块的空间维度小），但是延伸到输入块的全部深度。例如一个作用于第一层的大小为5x5x3的滤波器（5像素宽，5像素高，3个颜色通道）。在向前的方向上，我们用每个滤波器在输入块的空间维度上滑动（从左到右，从上到下）并计算滤波器和当前位置的点积。直观的，网络将会学习这些滤波器，它们会在“看到”（做点积）某些图片上的特征时激活，这些特征开始可能是一些局部特征，最后将会是一些整体特征。每一个滤波器会生成一个2维的激活图，我们在深度维度上堆叠这些激活图形成输出块。

**从大脑的角度分析** 如果你是一个大脑神经元分析的发烧友，3D输出块的每一个点可以解释为一个只看输入中一块小区域的神经元的输出，而空间上所有的神经元都共享参数。（因为这些结果都来自于一个相同的滤波器）我们现在讨论神经元连接，它们空间上的排列和它们共享参数的细节。

**局部连接** 当处理类似图片的高维度输入时，全连接是不可行的。取而代之的是只连接输入块的一个局部区域。这个连接的空间大小是一个叫做**接受域（receptive field）**的超参数（和滤波器大小相同），而连接的深度总是和输入块的深度相同。

![](http://cs231n.github.io/assets/cnn/depthcol.jpeg)

上图中，红色为一个大小为32x32x3的CIFAR-10图片的输入块，蓝色部分为一个卷积层的样例块。卷积层的每一个神经元只连接输入块空间上（宽和高）的一个局部区域，但是连接整个深度。换句话说，每个神经元就是一个滤波器和输入块上某一局部区域的点积结果。

![](http://cs231n.github.io/assets/nn1/neuron_model.jpeg)

普通神经网络中的神经元结构没变：依旧是计算权值和输入的乘积，然后跟随一个非线性单元，只是它们的链接被约束在一个局部区域。

**空间排列** 我们已经解释过CONV层中神经元和输入块之间的链接，但是还没有讨论输出块中有多少个神经元以及它们如何排列。三个超参数控制输出块的大小：**深度（depth），步长（stride）和0填充（zero-padding）**。

1. 首先，输出块的**深度**是一个超参数，它取决于我们想要用多少个滤波器，每一个滤波器学习去看输入中的不同东西。
2. 其次，我们必须设定**步长**用于滑动滤波器。步长为多少每次就滑动多少个像素，所以步长越大输出块越小。
3. 我们马上会看到，有时候为了方便我们需要在输入块周围填充0值，这个**0填充**的数量是一个超参数。0填充的好处是让我们能够控制输出块的大小。（最常见的是使输出块和输入块有相同的宽和高）

我们可以用一个函数计算输出块的空间大小，假设输入块大小为（\\(W\\)），CONV层接受域大小为（\\(F\\)），步长为（\\(S\\)），在边界上0填充的数量为（\\(P\\)），公式为\\((W - F + 2P)/S + 1\\)。

![](http://cs231n.github.io/assets/cnn/stride.jpeg)

图中是一个空间维度（宽）上的排列，一个神经元的接受域F=3，输入块宽度为W=5，0填充数量为P=1。左边的图是步长S=1的情况，输出块宽度为(5-3+2)/1+1=5。右边的图是步长S=2的情况，输出块宽度为(5-3+2)/2+1=3。值得注意的是步长S=3不能使用，因为(5-3+2)=4不能被3整除。这些黄色神经元的权值为[1,0,-1]，偏差为[0,0,0]，它们的参数是共享的。

**0填充的使用** 在步长\\(S=1\\)时设置0填充\\(P = (F - 1)/2\\)可以使输出块和输入块在空间上大小一致。

**步长的限制** 设置步长一定要使\\((W - F + 2P)/S\\)能够整除。

**共享参数** CONV的输出块在一个深度中共享权值（滤波器），权值大小就是接受域大小，输出块的深度就是滤波器的数目。

![](http://cs231n.github.io/assets/cnn/weights.jpeg)

上图中为96个大小为[11x11x3]的滤波器，输出块同一深度上的所有神经元共享它们中的一个。

注意有时候参数共享的假设可能没有意义。一个实际的例子是在人脸识别中，人脸在图片中央，而眼部特征、鼻子特征等都在不同的区域。这种情况下每个滤波器没有必要在整个输入块上滑动，此时CONV层变为**局部连接层（Locally-Connected Layer）**。

**Numpy例子** 假设输入块`X`大小为`X.shape:(11,11,4)`,0填充\\(P=0\\),滤波器\\(F=5\\)，步长\\(S=2\\)，这时输出块的空间大小为(11-5)/2+1=4，宽和高都为4。下面是输出块（V）的部分结果：

- `V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0`
- `V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0`
- `V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0`
- `V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0`

`W0`为第一个滤波器的权值，大小为`W0.shape: (5,5,4)`，第二个滤波器的部分结果为：

- `V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1`
- `V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1`
- `V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1`
- `V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1`

有多少个滤波器，输出块V的第三维深度就有多大。

**概要** 概括一下CONV层的概念：

- 输入块大小\\(W\_1 \times H\_1 \times D\_1\\)
- 需要4个超参数：
	- 滤波器数量\\(K\\)
	- 滤波器大小\\(F\\)
	- 步长\\(S\\)
	- 0填充数量\\(P\\) 
- 输出块大小为\\(W\_2 \times H\_2 \times D\_2\\)
	- \\(W\_2 = (W\_1 - F + 2P)/S + 1\\)
	- \\(H\_2 = (H\_1 - F + 2P)/S + 1\\)
	- \\(D\_2 = K\\)
- 每个滤波器有\\(F \cdot F \cdot D\_1\\)个权值，CONV层总共有\\((F \cdot F \cdot D\_1) \cdot K\\)个权值和\\(K\\)个偏差。
- 输出块在第\\(d\\)层深度上的切片（大小为\\(W\_2 \times H\_2\\)）是第\\(d\\)个滤波器以\\(S\\)的步长和偏差\\(d\\)在输入块上做卷积的结果。

一般的超参数设置为\\(F=3\\)，\\(S=1\\)，\\(P=1\\)。

![](http://images.cnblogs.com/cnblogs_com/tech0ne/1230148/o_conv.gif)

上图是一个用两个滤波器做卷积的例子。

**用矩阵乘法实现** 注意卷积操作本质上是将滤波器和输入的局部区域做点积，通常的实现方式如下：

1. 将输入的局部区域拉伸成一列，我们称为**im2col**操作。例如假设输入大小为[227x227x3]且将被大小为[11x11x3]的滤波器以步长4进行卷积。我们将输入中的每个将要做点积的[11x11x3]块拉伸为一个11*11*3=363的列，迭代这一操作得到长宽都为(227-11)/4+1=55的输出块`X_row`，大小为[3025x363]。注意每个大小为363的列中的数据有重叠。
2. 将每个大小为[11x11x3]的滤波器也拉伸成一列，假如有96个滤波器，那么得到输出`W_col`大小为[363x96]。
3. 然后将`X_row`和`W_col`做矩阵乘法`np.dot(X_row, W_col)`，得到输出大小为[3025x96]。
4. 最后将输出变形为[55x55x96]。

这种方法有个缺点是会使用很多内存，因为`X_row`中会有很多重复元素。尽管如此，它带来的好处是矩阵乘法有很多十分高效的实现（比如 [BLAS](http://www.netlib.org/blas/) API）。 而且im2col的这种思想在接下来的池化层也会用到。

**反向传播** 卷积操作的反向传播也是一个卷积（但是滤波器在空间上翻转了），这个结论可以简单的从1维的情况推导出（这里不展开）。

**1x1卷积** 1x1卷积的概念来自于[网络中的网络](https://arxiv.org/abs/1312.4400)，在CNNs中因为滤波器会延伸到输入块的整个深度，所以1x1卷积是有意义的。

**扩张卷积** 最近的一项关于[扩张卷积的研究](https://arxiv.org/abs/1511.07122)在CONV层引入了一个超参数**扩张数（dilation）**。如下图所示扩张卷积就是在滤波器的数据点之间插入空白，这样能够提高滤波器的感受野。（和池化作用类似还不丢失信息）

![](http://ocs628urt.bkt.clouddn.com/dilation-conv.png)

###2.2 池化层###
在CNNs架构中经常在连续的CONV层之间插入池化层，它的功能是逐渐降低空间大小来减少网络中的参数数量和计算量，并且也可以控制过拟合。池化层单独的在每个深度上进行操作，用MAX操作减小每个深度切片的空间大小。最常见的池化层用大小为2x2的滤波器以2为步长对每一个深度切片进行下采样，丢弃75%的输入数据。需要注意的是空间大小减小但深度不会改变。

**概要** 概括一下池化层的概念：

- 输入块大小\\(W\_1 \times H\_1 \times D\_1\\)
- 需要4个超参数：
	- 滤波器大小\\(F\\)
	- 步长\\(S\\)
- 输出块大小为\\(W\_2 \times H\_2 \times D\_2\\)
	- \\(W\_2 = (W\_1 - F)/S + 1\\)
	- \\(H\_2 = (H\_1 - F)/S + 1\\)
	- \\(D\_2 = D\_1\\)
- 没有引入参数因为它执行固定操作。
- 注意池化层一般不进行0填充。

值得注意的是最大池化层在实践中只有两种常用的变种：一种是\\(F=3\\)，\\(S=2\\)的重叠池化层（overlapping pooling），还有一种更常见的是\\(F=2\\)，\\(S=2\\)的池化层。池化用更大的接受域对数据破坏性太大。

**常规池化** 除了最大池化，池化层也可以进行其他形式的处理，比如平均池化，甚至L2范数池化。平均池化以前经常用，不过最近不流行了，因为最大池化操作在实践中有更好的表现。

![](http://cs231n.github.io/assets/cnn/maxpool.jpeg)

上图表示了大小为2x2，步长为2的滤波器在一个深度切片上的池化操作。

![](http://cs231n.github.io/assets/cnn/pool.jpeg)

上图表示了通过池化操作后，数据的空间大小减小而深度不变。

**反向传播** 回忆一下BP章节中对max(x,y)的操作是只需要传导输入时最大值的所在位置的梯度值。所以只需要在池化的正向传递时记录最大值的索引（有时我们叫它switches）就能够在BP时高效的计算梯度。

**去掉池化** 有很多人不喜欢池化操作而且觉得没有它也行。比如[Striving for Simplicity: The All Convolutional Net](http://arxiv.org/abs/1412.6806)提议抛弃池化层而改用只有CONV的架构。他们建议在CONV层中用更大的步长来缩小数据的大小。摒弃池化层被发现对训练好的生成模型来说很重要，比如变分自动编码器（VAEs）或者生成对抗网络（GANs）。未来的CNNs架构或许很少有甚至没有池化层。

###2.3 归一化层###
许多种类的归一化层被提出并使用在CNNs构架中，为了实现生物大脑中观察到的抑制状态。然而它们在实践中的贡献非常小，相关论述请看[cuda-convnet library API](http://cs231n.github.io/convolutional-networks/)。

###2.4 全连接层###
FC层中的神经元与上一层中的所有神经元相连，计算方式和普通神经网络的相关部分一样。

###2.5 FC层转化为CONV层###
值得注意的是FC层和CONV层的唯一区别就是CONV层的神经元共享参数并且只连接输入的一个局部区域。然而两种层仍然都是计算点积以至于函数形式是一样的。所以，结果是FC层和CONV层可以相互转换：

- 对于任何一个CONV层都有一个对应的FC层实现同样的前向函数。它的权值矩阵是一个大多都是0的大矩阵，其中只有特定的区域有值（因为局部连接），还有很多个区域的权值相同（因为参数共享）。
- 相反的任何一个FC层都能转化为CONV层。例如一个\\(K=4096\\)的FC层面对一个\\(7x7x512\\)的输入块可以等价表示为一个\\(F=7,P=0,S=1,K=4096\\)的CONV层。换句话说，我们**设置滤波器大小和输入块大小一样**，那么输出块将会是1x1x4096。

**FC->CONV 转换** 对于这两种转换，FC层向CONV层的转换在实践中特别有用。想一下一个输入为大小为224x224x3的图片的CNNs，然后用一系列的CONV层和POOL层来将图片缩小为大小为7x7x512的输出块（后面将会看到的AlexNet架构，每次下采样输入缩减一半数据，224/2/2/2/2/2= 7）。然后AlexNet用两个大小为4096的FC层，最后用一个FC层用1000个神经元计算类型得分。我们可以把这三个FC层分别用上述方法转换为CONV层：

- 将第一个面对[7x7x512]输入块的FC层换为滤波器为\\(F=7\\)的CONV层并得到输出块[1x1x4096]。
- 将第二个FC层换为滤波器为\\(F=1\\)的CONV层，得到输出块[1x1x4096]。
- 将最后的FC层换为滤波器为\\(F=1\\)的CONV层，得到输出块[1x1x1000]。

每一个这种转换在实践中的操作是将FC层中的权值矩阵\\(W\\)变形成CONV层中的多个滤波器。这种转换的结果是可以让我们在一次前向传播中得到一张更大图片的多个空间位置的得分。

例如，如果224x224的图片产生的输出为[7x7x512]，那么处理384x384的图片就会产生大小为[12x12x512]的输出（384/32=12）。接下经过来我们刚才从FC层变换而来的CONV层会最终产生大小为[6x6x1000]的输出，因为(12-7)/1+1=6。注意不同于224x224图片得到的为一组标量类型得分，384x384图片得到的是一组大小为6x6的类型得分。

用原始的CNNs（FC没有转换为CONV）以32为步长对384x384图片中的多个224x224区域进行评价的结果和用转换后的CNNs进行评价的结果一致。（这个地方原文比较难懂，其实可以看作是用224滤波器卷积384图片，输出大小为（384-224）/32 + 1 = 6，所以384图片的单个类型得分大小为6x6）

自然的，转换后的CNNs前向传播一次比原始CNNs在36个位置传播一次要高效，因为36次计算是共享参数的。这种技巧经常在实践用于提高性能，例如我们经常放大一张图片（其中很多数据是冗余的），然后用一个转换后的CNNs去评价在多个空间位置上的得分然后取平均值。

最后，当步长小于32像素时如果我们想要高效的处理图片该做什么？我们可以用多次前向传播解决这个问题。比如我们想要用16像素的步长可以这样做：将两次由转换后CNNs前向传播的输出合并，前一次使用原始图片，第二次使用在宽和高上都平移了16个像素的图片。

- 一个IPython Notebook的例子[Net Surgery](https://github.com/BVLC/caffe/blob/master/examples/net_surgery.ipynb)展示了这种变换的代码（用Caffe）。

##3.CNNs架构##
我们已经看到CNNs一般由三种类型的层构成：CONV，POOL（不特殊申明一般指最大池化）和FC。我们会显式的将RELU激活作为一层，它对所有元素进行非线性操作。在这一部分我们讨论如何堆叠这些层来构建整个CNNs。

###3.1 层排列###
最常见的CNNs形式是堆叠一些CONV-RELU层，在它们后面添加POOL层，然后不断重复这种模式直到图片被从空间上合并到一个小的尺度。在这之后，经常需要接上一些全连接层，最后一个全连接层保存像类型得分之类的输出。最常见的CNNs架构遵从下面的模式：

    INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC

其中的`*`代表重复，`?`代表可选项。一般情况下:

	N >= 0 && N <=3, M >= 0, K >= 0 && K < 3

下面是一些遵循上述模式的基本CNNs架构：

- `INPUT -> FC`，线性分类器。
- `INPUT -> [FC -> RELU]*K -> FC`，常规的K隐层神经网络。
- `INPUT -> CONV -> RELU - FC`
- `INPUT -> [CONV -> RELU -> POOL]*2 -> FC -> RELU -> FC`
- `INPUT -> [[CONV -> RELU]*2 -> POOL]*3 -> [FC -> RELU]*2 -> FC`

使用一堆小滤波器的CONV层而不是一个大滤波器的CONV层。

假设你堆叠三个3x3的CONV层（当前每层之间有非线性单元），在这种排列下第一层CONV的每个神经元对输入有3x3的视野。第二层CONV的神经元对第一层CONV有3x3的视野，因此对输入有5x5的视野。类似的，一个第三层CONV的神经元对第二层CONV有3x3的视野，对输入有7x7的视野。

假设不用三个3x3的CONV层，而直接使用一个单独的7x7的CONV层，这样会有很多缺点。首先，这些神经元对输入做线性操作，而三个CONV层含有非线性单元操作使得它们更有表现力。第二，如果我们假设有C个通道，那么7x7的CONV层有\\(C \times (7 \times 7 \times C) = 49 C^2\\)个参数，而三个3x3的CONV层只有\\(3 \times (C \times (3 \times 3 \times C) = 27 C^2\\)个参数。这种方式也有一个缺点，在实践中需要更多的内存在存储BP时中间CONV层的结果。

**非线性的层排列** 应该注意的是传统的线性排列层的模式已经被挑战了，在Google的Inception构架中和最近的（最先进的）残差网络（Residual Network)中，都采用了与线性排列不同的且更复杂的层连接结构。

**在实践用什么架构在ImageNet中效果最好** 如果你已经对思考架构选择感到疲劳了，可能会对下面的结论感到高兴，其实90%甚至更多的应用不需要考虑这些问题。对这个问题的总结为“不要逞英雄”：当你在对问题采用什么架构摇摆不定时，应该看看当前ImageNet上表现最好的架构，下载预训练模型然后在你的数据上进行微调。你应该基本没有必要去从头开始训练CNNs或者从头设计CNNs。我也是从[Deep Learning school](https://www.youtube.com/watch?v=u6aEYuemt0M)了解这一点的。

###3.2 层定制###
到目前为止我们没有提及CNNs每类层中常用的超参数。我们首先将会论述一些定制层的常用规则，随后是一些关于规则的讨论：

- 输入层的图片数据应该是能够被2整除的。常见的大小包括32（CIFAR-10），64，96（STL-10），224（ImageNet），384，512。
- 卷积层应该使用小的滤波器（3x3或者最多5x5），步长\\(S=1\\)，重要的是设置0填充使输出和输入在空间上大小一致（\\(P=(F-1)/2\\)）。如果你非要用大滤波器，那么常用的只有在最开始的CONV层中用于查看图片。
- 池化层负责下采样输入的空间维度数据。最常见的最大池化设置是\\(F=2\\)的接受域和\\(S=2\\)的步长，它每次丢弃输入中75%的数据（宽和高都减少1/2）。另一种稍微少见一些的设置是\\(F=3\\)的接受域和\\(S=2\\)的步长。大于3的接受域会带来太多的数据损失并过于激进，一般表现很差。

*减少定制层的麻烦。* 上述方案是令人愉快的，因为CONV层保持了输入的空间大小，POOL层单独的负责在空间上下采样数据。如果我们在CONV层中用大于1的步长而且不用0填充，那就要特别小心CNNs中每层的输出，保证所有的步长和滤波器能够协同工作。

*CONV为什么使用为1的步长？* 小的步长在实践中效果更好，而且已经提到过步长为1可以使所有空间上下采样的工作都交给POOL层，CONV层只变换改变输入块的深度。

*为什么使用0填充？* 除了之前提到的可以在CONV层处理后保持空间大小之外，其实还可以提高性能。如果CONV层不进行0填充而只做有效卷积，那么在CONV后数据会减小，数据边界处的信息会很快被“洗掉”。

*对内存限制的妥协。* 在某种情况下，使用上述经验使内存消耗增长非常快。例如用64个3x3滤波器且0填充为1的CONV层处理224x224x3的图片，应该生成3D输出块大小为[224x224x64]。这大概有1000万个参数和300万个输出元素，换句话说要消耗大概99MB的内存。因为通常GPU的瓶颈来自内存，所以有必要进行妥协。在实践中，人们一般只在CNNs的第一个CONV层进行妥协，例如ZFNet中的7x7滤波器和步长2，AlexNet中的11x11滤波器和步长4。

###3.3 案例学习###
下面这些是在CNNs领域中著名的架构：

- **LeNet** 第一个CNNs的成功应用，由Yann LeCun在1990年开发。[LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)广为人知是它能识别邮编和数字等信息。
- **AlexNet** [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)是使CNNs在计算机视觉领域火起来的第一个成果，由Alex Krizhevsky，Ilya Sutskever和Geoff Hinton共同开发。AlexNet参加了2012年的[ImageNet ILSVRC challenge](http://www.image-net.org/challenges/LSVRC/2012/)并以巨大的优势夺冠（top5error为16%，第二名为26%）。这个网络拥有比LeNet小很多的架构，但是层次更深更大且堆叠多个CONV（这之前的CNNs在一个CONV层后就马上连接一个POOL层）。
- **ZFNet** ILSVRC 2013的冠军是Matthew Zeiler和Rob Fergus开发的CNNs，一般被称为ZFNet。这个网络对AlexNet进行了改进，调整了架构的超参数，加大了中间的CONV层的大小并减小第一层的步长和滤波器大小。
- **GooLeNet** ILSVRC 2014的冠军是来自Google的[Szegedy团队](https://arxiv.org/abs/1409.4842)。这个网络主要的贡献是开发了Inception模块使网络中的参数大幅减少（4M对比AlexNet中的60M）。另外用平均池化代替了CNNs顶部的FC层，进一步消灭了大量似乎不重要的参数。这里有一些GooLeNet的后续版本，最新的是[Inception-v4](http://arxiv.org/abs/1602.07261)。
- **VGGNet** ILSVRC 2014的亚军是Karen Simonyan和Andrew Zisserman开发的[VGGNet](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)。这个网络的主要贡献是展示了网络深度是获得好的性能的关键因素。它含有16个CONV/FC层，并从头到尾使用3x3的CONV和2x2的POOL，这个预训练模型已经集成到了Caffe中。VGGNet的缺点是用更大的开销去使用了更多的内存和参数（140M），其中大多数参数都在第一个FC层中（后来发现这些FC层都可以在不降低性能的情况下移除，极大的减少了参数）。
- **ResNet** [残差网络（Residual Network）](https://arxiv.org/abs/1512.03385)由ILSVRC 2015的冠军何恺明团队开发。这个网络的特点是一些特别的跨层连接和对[batch normalization](http://arxiv.org/abs/1502.03167)的大量使用。这个架构也在网络的最后去掉了FC层。你也可以参考何恺明的演示（[视频](https://www.youtube.com/watch?v=1PGLj-uKT1w)和[PPT](http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf)）和一些用Torch做的[近期实验](https://github.com/gcr/torch-residual-networks)。ResNet是目前为止（2016）最先进的CNNs并且是实践CNNs的默认选择。最后，也可以从[Kaiming He et al. Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)得到对这个架构的最新研究（2016年3月）。

**VGGNet的细节** 让我们分析VGGNet的更多细节作为案例教学。整个VGGNet由大小3x3、步长为1、0填充为1的CONV层和大小2x2、步长为2的POOL层组成。我们可以把每一步的数据大小写下来，并追踪数据大小和参数总数：

	INPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0
	CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728
	CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864
	POOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0
	CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728
	CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456
	POOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0
	CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912
	CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
	CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
	POOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0
	CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648
	CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
	CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
	POOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0
	CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
	CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
	CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
	POOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0
	FC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448
	FC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216
	FC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000
	
	TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)
	TOTAL params: 138M parameters

在CNNs常见的是大多数的内存开销（计算时间也一样）是使用在开始的几个CONV层中，大多数的参数都在最后几个FC层中。在上面的例子中，第一个FC层包含1亿个权值，整个网络权值大概总共1.4亿个。

###3.4 计算的注意事项###


---
© 2018 by 0ne.tech